{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXIZGbfvry6I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from csv file\n",
        "data = np.genfromtxt('/content/data.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "Tsek51B_t34Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features (X) and labeles (y)\n",
        "X = data[1:, 1:3]\n",
        "y = data[1:, 3]"
      ],
      "metadata": {
        "id": "T5qU3Z8TwnO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets (80/20)\n",
        "indices = np.random.permutation(X.shape[0])\n",
        "training_indices = indices[:int(X.shape[0] * 0.8)]\n",
        "testing_indices = indices[int(X.shape[0] * 0.8):]\n",
        "X_train, X_test = X[training_indices], X[testing_indices]\n",
        "y_train, y_test = y[training_indices], y[testing_indices]"
      ],
      "metadata": {
        "id": "oUa52FjRzdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model parameters\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0"
      ],
      "metadata": {
        "id": "nA2wWp6w2e0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "TorASz8S3E8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic loss function (E)\n",
        "def logistic_loss(y, y_pred):\n",
        "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
      ],
      "metadata": {
        "id": "upaduOix3bHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the gradient of the logistic loss function\n",
        "def logistic_loss_grad(X, y, y_pred):\n",
        "    return np.dot(X.T, y_pred - y) / y.shape[0]"
      ],
      "metadata": {
        "id": "PG69jERN4yEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of iterations and the learning rate\n",
        "n_iter = 1000\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "URdPXhg48vHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform SGD on training data\n",
        "for i in range(n_iter):\n",
        "    # Compute the predicted probabilities\n",
        "    y_pred = sigmoid(np.dot(X_train, w) + b)\n",
        "\n",
        "    # Compute the gradients\n",
        "    grad_w = logistic_loss_grad(X_train, y_train, y_pred)\n",
        "    grad_b = np.mean(y_pred - y_train)\n",
        "\n",
        "    # Update the parameters\n",
        "    w -= learning_rate * grad_w\n",
        "    b -= learning_rate * grad_b"
      ],
      "metadata": {
        "id": "_FqywnFQ9exE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final model parameters\n",
        "print('parameters:', b, w)\n",
        "# Minimal value of logistic loss function (value for this parameters)\n",
        "y_pred = sigmoid(np.dot(X_train, w) + b)\n",
        "print('logistic loss = ', logistic_loss(y_train,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vCGdyxetx3J",
        "outputId": "784888b4-0f3f-4e7b-94b5-faed9060ae46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters: 3.4006245575585083 [ 1.30767074 -0.83813944]\n",
            "logistic loss =  0.06530358158077788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing model on testing set\n",
        "\n",
        "y_pred_test = sigmoid(np.dot(X_test, w) + b)\n",
        "y_end = (y_pred_test > 0.5).astype(int)\n",
        "y_true = y_test.astype(int)\n",
        "\n",
        "# Confusion matrix\n",
        "\n",
        "confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "for i in range(y_true.shape[0]):\n",
        "  confusion_matrix[y_true[i], y_end[i]] += 1.0\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9Mmw0F0uRdK",
        "outputId": "6ec40597-cdb0-4c76-ad04-2b996e62de73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            "[[ 96   1]\n",
            " [  3 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy, precision, recall\n",
        "TP = confusion_matrix[0, 0]\n",
        "FP = confusion_matrix[0, 1]\n",
        "FN = confusion_matrix[1, 0]\n",
        "TN = confusion_matrix[1, 1]\n",
        "\n",
        "acc = (TP + TN) / (TP + FP + FN + TN)\n",
        "prec = TP / (TP + FP)\n",
        "rec = TP / (TP + FN)\n",
        "\n",
        "print('Accuracy: ', acc)\n",
        "print('Precision: ', prec)\n",
        "print('Recall: ', rec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5aD-KU2eZxk",
        "outputId": "ea1c9e7a-a8a1-4a3f-8c09-34840c456838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.95\n",
            "Precision:  0.9901960784313726\n",
            "Recall:  0.9181818181818182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Omówienie i wnioski\n",
        "\n",
        "W powyższym projekcie zbudowałem model regresji logistycznje przy użyciu biblioteki NumPy. Do optymalizacji działania medelu użyłem metody SGD (gradient descent).\n",
        "\n",
        "Początkowy zbiór danych podzieliłem na dwa podzbiory: treningowy i testowy. \n",
        "\n",
        "Podziału dokonałem w proporcji 80 do 20, tzn. 80% danych trafiło do zbioru testowego a 20% do zbioru treningowego.\n",
        "\n",
        "Następnie wykorzystałem zbiór treningowy w celu wyszkolenia modelu, tzn. wyznaczenia parametrów w0, w1 i w2 (w0 = b), dla których funkcja straty osiąga swoje minumum (bądź jest jego blisko).\n",
        "\n",
        "Model z wyznaczonymi w ten sposób parametrami powinien radzić sobie najlepiej z zadaniem klasywikacji dla naszych danych.\n",
        "\n",
        "Następnie w celu sprawdzenia jakości naszego modelu przeprowadziłem testy tego modelu przy użyciu odłożonych wcześnij danych testowych.\n",
        "\n",
        "Dokonałem klasyfikacji punktów z tego zbiotu przy użyciu modelu a następnie porównałem wt wyniki z realnymi etykietami.\n",
        "\n",
        "Na koniec sporządziłem macież pomyłek, oraz obliczyłem wskaźniki dokładności, precyzji i czułości.\n",
        "\n",
        "Na ich podstawie można stwierdzić, że zbudowany model radzi sobie z zadaniem klasyfikacji całkiem dobrze (prawidłowo przyporządkowuje kategorie w >95% przypadków). Można też stwierdzić, że cechuje się on dużo lepszą precyzją niz czułością."
      ],
      "metadata": {
        "id": "s4DigTHQomLX"
      }
    }
  ]
}